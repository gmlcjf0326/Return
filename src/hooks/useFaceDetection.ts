'use client';

import { useState, useEffect, useCallback, useRef } from 'react';
import { initTensorFlow } from '@/lib/ai/tensorflow';

// ê°ì • íƒ€ì…
export type EmotionType =
  | 'neutral'    // ì¤‘ë¦½/ì§‘ì¤‘
  | 'happy'      // í–‰ë³µ/ë§Œì¡±
  | 'confused'   // í˜¼ë€/ë‹¹í™©
  | 'anxious'    // ë¶ˆì•ˆ/ê¸´ì¥
  | 'sad'        // ìŠ¬í””
  | 'surprised'  // ë†€ëŒ
  | 'angry';     // í™”ë‚¨

// ê°ì • ê¸°ë¡
export interface EmotionRecord {
  timestamp: number;
  emotion: EmotionType;
  confidence: number;
  questionIndex?: number;
}

// ê°ì • ë¶„í¬ í†µê³„
export interface EmotionDistribution {
  emotion: EmotionType;
  count: number;
  percentage: number;
}

// ì–¼êµ´ ê°ì§€ ê²°ê³¼
export interface FaceDetectionResult {
  isDetected: boolean;
  emotion: EmotionType;
  confidence: number;
  landmarks?: {
    leftEye: { x: number; y: number };
    rightEye: { x: number; y: number };
    nose: { x: number; y: number };
    mouth: { x: number; y: number };
  };
}

// í›… ì˜µì…˜
interface UseFaceDetectionOptions {
  enabled?: boolean;
  detectionInterval?: number; // ms
  onEmotionChange?: (emotion: EmotionType, confidence: number) => void;
}

// í›… ë°˜í™˜ íƒ€ì…
interface UseFaceDetectionReturn {
  isLoading: boolean;
  isActive: boolean;
  isPermissionGranted: boolean;
  currentEmotion: EmotionType;
  emotionTimeline: EmotionRecord[];
  emotionDistribution: EmotionDistribution[];
  dominantEmotion: EmotionType;
  videoRef: React.RefObject<HTMLVideoElement | null>;
  startDetection: () => Promise<boolean>;
  stopDetection: () => void;
  clearTimeline: () => void;
  recordEmotionForQuestion: (questionIndex: number) => void;
}

export function useFaceDetection(options: UseFaceDetectionOptions = {}): UseFaceDetectionReturn {
  const {
    enabled = true,
    detectionInterval = 1000,
    onEmotionChange,
  } = options;

  const [isLoading, setIsLoading] = useState(false);
  const [isActive, setIsActive] = useState(false);
  const [isPermissionGranted, setIsPermissionGranted] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState<EmotionType>('neutral');
  const [emotionTimeline, setEmotionTimeline] = useState<EmotionRecord[]>([]);

  const videoRef = useRef<HTMLVideoElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const detectionIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const currentQuestionIndexRef = useRef<number>(0);

  // ê°ì • ë¶„í¬ ê³„ì‚°
  const emotionDistribution = calculateEmotionDistribution(emotionTimeline);

  // ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚œ ê°ì •
  const dominantEmotion = emotionDistribution.length > 0
    ? emotionDistribution[0].emotion
    : 'neutral';

  // ì›¹ìº  ìŠ¤íŠ¸ë¦¼ ì‹œì‘
  const startWebcam = useCallback(async (): Promise<boolean> => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640 },
          height: { ideal: 480 },
          facingMode: 'user',
        },
        audio: false,
      });

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        await videoRef.current.play();
      }

      streamRef.current = stream;
      setIsPermissionGranted(true);
      return true;
    } catch (error) {
      console.error('[FaceDetection] Webcam access denied:', error);
      setIsPermissionGranted(false);
      return false;
    }
  }, []);

  // ì›¹ìº  ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€
  const stopWebcam = useCallback(() => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
  }, []);

  // ê°„ë‹¨í•œ í‘œì • ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ëª¨ë¸ ì‚¬ìš©)
  // ì—¬ê¸°ì„œëŠ” ëœë¤ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” face-landmarks-detection ëª¨ë¸ ì‚¬ìš©)
  const analyzeEmotion = useCallback((): EmotionType => {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ ë¶„ì„í•˜ì—¬ ê°ì • ì¶”ë¡ 
    // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
    const emotions: EmotionType[] = ['neutral', 'happy', 'confused', 'anxious', 'surprised'];
    const weights = [0.5, 0.15, 0.15, 0.1, 0.1]; // ì¤‘ë¦½ì´ ê°€ì¥ ë§ì´ ë‚˜ì˜¤ë„ë¡

    const random = Math.random();
    let cumulative = 0;

    for (let i = 0; i < emotions.length; i++) {
      cumulative += weights[i];
      if (random < cumulative) {
        return emotions[i];
      }
    }

    return 'neutral';
  }, []);

  // ê°ì • ê°ì§€ ë£¨í”„
  const detectEmotion = useCallback(() => {
    if (!isActive || !videoRef.current) return;

    const emotion = analyzeEmotion();
    const confidence = 0.7 + Math.random() * 0.25; // 70-95% ì‹ ë¢°ë„

    setCurrentEmotion(emotion);

    // íƒ€ì„ë¼ì¸ì— ê¸°ë¡
    const record: EmotionRecord = {
      timestamp: Date.now(),
      emotion,
      confidence,
      questionIndex: currentQuestionIndexRef.current,
    };

    setEmotionTimeline(prev => [...prev, record]);

    // ì½œë°± í˜¸ì¶œ
    if (onEmotionChange) {
      onEmotionChange(emotion, confidence);
    }
  }, [isActive, analyzeEmotion, onEmotionChange]);

  // ê°ì§€ ì‹œì‘
  const startDetection = useCallback(async (): Promise<boolean> => {
    if (!enabled) return false;

    setIsLoading(true);

    try {
      // TensorFlow.js ì´ˆê¸°í™”
      await initTensorFlow();

      // ì›¹ìº  ì‹œì‘
      const webcamStarted = await startWebcam();
      if (!webcamStarted) {
        setIsLoading(false);
        return false;
      }

      setIsActive(true);
      setIsLoading(false);

      // ê°ì§€ ì¸í„°ë²Œ ì‹œì‘
      detectionIntervalRef.current = setInterval(detectEmotion, detectionInterval);

      return true;
    } catch (error) {
      console.error('[FaceDetection] Failed to start:', error);
      setIsLoading(false);
      return false;
    }
  }, [enabled, startWebcam, detectEmotion, detectionInterval]);

  // ê°ì§€ ì¤‘ì§€
  const stopDetection = useCallback(() => {
    setIsActive(false);

    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
      detectionIntervalRef.current = null;
    }

    stopWebcam();
  }, [stopWebcam]);

  // íƒ€ì„ë¼ì¸ ì´ˆê¸°í™”
  const clearTimeline = useCallback(() => {
    setEmotionTimeline([]);
  }, []);

  // íŠ¹ì • ë¬¸í•­ì˜ ê°ì • ê¸°ë¡
  const recordEmotionForQuestion = useCallback((questionIndex: number) => {
    currentQuestionIndexRef.current = questionIndex;
  }, []);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopDetection();
    };
  }, [stopDetection]);

  // ê°ì§€ ë£¨í”„ ì—…ë°ì´íŠ¸
  useEffect(() => {
    if (isActive && !detectionIntervalRef.current) {
      detectionIntervalRef.current = setInterval(detectEmotion, detectionInterval);
    }

    return () => {
      if (detectionIntervalRef.current) {
        clearInterval(detectionIntervalRef.current);
        detectionIntervalRef.current = null;
      }
    };
  }, [isActive, detectEmotion, detectionInterval]);

  return {
    isLoading,
    isActive,
    isPermissionGranted,
    currentEmotion,
    emotionTimeline,
    emotionDistribution,
    dominantEmotion,
    videoRef,
    startDetection,
    stopDetection,
    clearTimeline,
    recordEmotionForQuestion,
  };
}

// ê°ì • ë¶„í¬ ê³„ì‚° ìœ í‹¸ë¦¬í‹°
function calculateEmotionDistribution(timeline: EmotionRecord[]): EmotionDistribution[] {
  if (timeline.length === 0) return [];

  const counts: Record<EmotionType, number> = {
    neutral: 0,
    happy: 0,
    confused: 0,
    anxious: 0,
    sad: 0,
    surprised: 0,
    angry: 0,
  };

  timeline.forEach(record => {
    counts[record.emotion]++;
  });

  const total = timeline.length;

  return Object.entries(counts)
    .filter(([, count]) => count > 0)
    .map(([emotion, count]) => ({
      emotion: emotion as EmotionType,
      count,
      percentage: Math.round((count / total) * 100),
    }))
    .sort((a, b) => b.count - a.count);
}

// ê°ì • ì´ë¦„ í•œê¸€í™”
export const emotionLabels: Record<EmotionType, string> = {
  neutral: 'ì§‘ì¤‘',
  happy: 'ë§Œì¡±',
  confused: 'í˜¼ë€',
  anxious: 'ë¶ˆì•ˆ',
  sad: 'ìŠ¬í””',
  surprised: 'ë†€ëŒ',
  angry: 'í™”ë‚¨',
};

// ê°ì • ì•„ì´ì½˜
export const emotionIcons: Record<EmotionType, string> = {
  neutral: 'ğŸ˜',
  happy: 'ğŸ˜Š',
  confused: 'ğŸ˜•',
  anxious: 'ğŸ˜°',
  sad: 'ğŸ˜¢',
  surprised: 'ğŸ˜®',
  angry: 'ğŸ˜ ',
};

// ê°ì • ìƒ‰ìƒ
export const emotionColors: Record<EmotionType, string> = {
  neutral: '#6B7280',
  happy: '#10B981',
  confused: '#F59E0B',
  anxious: '#EF4444',
  sad: '#3B82F6',
  surprised: '#8B5CF6',
  angry: '#DC2626',
};

export default useFaceDetection;
