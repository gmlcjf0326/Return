'use client';

import { useState, useEffect, useCallback, useRef } from 'react';
import { initTensorFlow } from '@/lib/ai/tensorflow';
import * as faceLandmarksDetection from '@tensorflow-models/face-landmarks-detection';

// ê°ì • íƒ€ì…
export type EmotionType =
  | 'neutral'    // ì¤‘ë¦½/ì§‘ì¤‘
  | 'happy'      // í–‰ë³µ/ë§Œì¡±
  | 'confused'   // í˜¼ë€/ë‹¹í™©
  | 'anxious'    // ë¶ˆì•ˆ/ê¸´ì¥
  | 'sad'        // ìŠ¬í””
  | 'surprised'  // ë†€ëŒ
  | 'angry';     // í™”ë‚¨

// ê°ì • ê¸°ë¡
export interface EmotionRecord {
  timestamp: number;
  emotion: EmotionType;
  confidence: number;
  questionIndex?: number;
}

// ê°ì • ë¶„í¬ í†µê³„
export interface EmotionDistribution {
  emotion: EmotionType;
  count: number;
  percentage: number;
}

// ì–¼êµ´ ê°ì§€ ê²°ê³¼
export interface FaceDetectionResult {
  isDetected: boolean;
  emotion: EmotionType;
  confidence: number;
  landmarks?: {
    leftEye: { x: number; y: number };
    rightEye: { x: number; y: number };
    nose: { x: number; y: number };
    mouth: { x: number; y: number };
  };
}

// í›… ì˜µì…˜
interface UseFaceDetectionOptions {
  enabled?: boolean;
  detectionInterval?: number; // ms
  onEmotionChange?: (emotion: EmotionType, confidence: number) => void;
}

// í›… ë°˜í™˜ íƒ€ì…
interface UseFaceDetectionReturn {
  isLoading: boolean;
  isActive: boolean;
  isPermissionGranted: boolean;
  currentEmotion: EmotionType;
  emotionTimeline: EmotionRecord[];
  emotionDistribution: EmotionDistribution[];
  dominantEmotion: EmotionType;
  error: string | null;
  videoRef: React.RefObject<HTMLVideoElement | null>;
  startDetection: () => Promise<boolean>;
  stopDetection: () => void;
  clearTimeline: () => void;
  recordEmotionForQuestion: (questionIndex: number) => void;
}

export function useFaceDetection(options: UseFaceDetectionOptions = {}): UseFaceDetectionReturn {
  const {
    enabled = true,
    detectionInterval = 1000,
    onEmotionChange,
  } = options;

  const [isLoading, setIsLoading] = useState(false);
  const [isActive, setIsActive] = useState(false);
  const [isPermissionGranted, setIsPermissionGranted] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState<EmotionType>('neutral');
  const [emotionTimeline, setEmotionTimeline] = useState<EmotionRecord[]>([]);
  const [error, setError] = useState<string | null>(null);

  const videoRef = useRef<HTMLVideoElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const detectionIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const currentQuestionIndexRef = useRef<number>(0);
  const detectorRef = useRef<faceLandmarksDetection.FaceLandmarksDetector | null>(null);

  // ê°ì • ë¶„í¬ ê³„ì‚°
  const emotionDistribution = calculateEmotionDistribution(emotionTimeline);

  // ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚œ ê°ì •
  const dominantEmotion = emotionDistribution.length > 0
    ? emotionDistribution[0].emotion
    : 'neutral';

  // ì›¹ìº  ìŠ¤íŠ¸ë¦¼ ì‹œì‘
  const startWebcam = useCallback(async (): Promise<boolean> => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640 },
          height: { ideal: 480 },
          facingMode: 'user',
        },
        audio: false,
      });

      // ë¹„ë””ì˜¤ íŠ¸ë™ ì„¤ì • ë¡œê·¸
      const videoTrack = stream.getVideoTracks()[0];
      if (videoTrack) {
        const settings = videoTrack.getSettings();
        console.log('[FaceDetection] Video track settings:', settings);
      }

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        // ê·¸ë ˆì´ìŠ¤ì¼€ì¼ í•„í„° ë°©ì§€
        videoRef.current.style.filter = 'none';
        await videoRef.current.play();
      }

      streamRef.current = stream;
      setIsPermissionGranted(true);
      return true;
    } catch (err) {
      console.error('[FaceDetection] Webcam access denied:', err);
      setError('ì¹´ë©”ë¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
      setIsPermissionGranted(false);
      return false;
    }
  }, []);

  // ì›¹ìº  ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€
  const stopWebcam = useCallback(() => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
  }, []);

  // ì–¼êµ´ ëœë“œë§ˆí¬ ê¸°ë°˜ ì‹¤ì œ í‘œì • ë¶„ì„
  const analyzeEmotionFromLandmarks = useCallback((keypoints: faceLandmarksDetection.Keypoint[]): EmotionType => {
    if (keypoints.length < 468) {
      return 'neutral';
    }

    // MediaPipe FaceMesh ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (468ê°œ í¬ì¸íŠ¸)
    // ì… ê´€ë ¨ ëœë“œë§ˆí¬
    const upperLipTop = keypoints[13];      // ìœ—ì…ìˆ  ìœ„
    const lowerLipBottom = keypoints[14];   // ì•„ë«ì…ìˆ  ì•„ë˜
    const mouthLeft = keypoints[61];        // ì… ì™¼ìª½ ë
    const mouthRight = keypoints[291];      // ì… ì˜¤ë¥¸ìª½ ë
    const upperLipCenter = keypoints[0];    // ìœ—ì…ìˆ  ì¤‘ì•™

    // ëˆˆ ê´€ë ¨ ëœë“œë§ˆí¬
    const leftEyeTop = keypoints[159];      // ì™¼ìª½ ëˆˆ ìœ„
    const leftEyeBottom = keypoints[145];   // ì™¼ìª½ ëˆˆ ì•„ë˜
    const rightEyeTop = keypoints[386];     // ì˜¤ë¥¸ìª½ ëˆˆ ìœ„
    const rightEyeBottom = keypoints[374];  // ì˜¤ë¥¸ìª½ ëˆˆ ì•„ë˜

    // ëˆˆì¹ ê´€ë ¨ ëœë“œë§ˆí¬
    const leftEyebrowInner = keypoints[107];   // ì™¼ìª½ ëˆˆì¹ ì•ˆìª½
    const leftEyebrowOuter = keypoints[70];    // ì™¼ìª½ ëˆˆì¹ ë°”ê¹¥ìª½
    const rightEyebrowInner = keypoints[336];  // ì˜¤ë¥¸ìª½ ëˆˆì¹ ì•ˆìª½
    const rightEyebrowOuter = keypoints[300];  // ì˜¤ë¥¸ìª½ ëˆˆì¹ ë°”ê¹¥ìª½

    // ì½” ëœë“œë§ˆí¬ (ê¸°ì¤€ì )
    const noseTip = keypoints[4];

    // ê±°ë¦¬ ê³„ì‚° ìœ í‹¸ë¦¬í‹°
    const distance = (p1: faceLandmarksDetection.Keypoint, p2: faceLandmarksDetection.Keypoint) => {
      return Math.sqrt(Math.pow(p1.x - p2.x, 2) + Math.pow(p1.y - p2.y, 2));
    };

    // ì–¼êµ´ í¬ê¸° ì •ê·œí™”ë¥¼ ìœ„í•œ ê¸°ì¤€ ê±°ë¦¬ (ëˆˆ ì‚¬ì´ ê±°ë¦¬)
    const leftEyeCenter = keypoints[33];
    const rightEyeCenter = keypoints[263];
    const faceWidth = distance(leftEyeCenter, rightEyeCenter);

    if (faceWidth < 10) return 'neutral'; // ì–¼êµ´ì´ ë„ˆë¬´ ì‘ìŒ

    // 1. ì… ë²Œë¦¼ ì •ë„ (ì… ë†’ì´ / ì–¼êµ´ ë„ˆë¹„)
    const mouthOpenRatio = distance(upperLipTop, lowerLipBottom) / faceWidth;

    // 2. ë¯¸ì†Œ ì •ë„ (ì… ë„ˆë¹„ / ì–¼êµ´ ë„ˆë¹„ + ì…ê¼¬ë¦¬ ë†’ì´)
    const mouthWidth = distance(mouthLeft, mouthRight);
    const mouthWidthRatio = mouthWidth / faceWidth;
    const mouthCornerAvgY = (mouthLeft.y + mouthRight.y) / 2;
    const upperLipY = upperLipCenter.y;
    const smileRatio = (upperLipY - mouthCornerAvgY) / faceWidth; // ì–‘ìˆ˜ë©´ ë¯¸ì†Œ

    // 3. ëˆˆ í¬ê²Œ ëœ¸ ì •ë„
    const leftEyeOpen = distance(leftEyeTop, leftEyeBottom) / faceWidth;
    const rightEyeOpen = distance(rightEyeTop, rightEyeBottom) / faceWidth;
    const eyeOpenRatio = (leftEyeOpen + rightEyeOpen) / 2;

    // 4. ëˆˆì¹ ì°Œí‘¸ë¦¼ ì •ë„ (ëˆˆì¹ ì•ˆìª½ì´ ë‚´ë ¤ê°)
    const leftBrowFrown = (leftEyebrowInner.y - leftEyeTop.y) / faceWidth;
    const rightBrowFrown = (rightEyebrowInner.y - rightEyeTop.y) / faceWidth;
    const browFrownRatio = (leftBrowFrown + rightBrowFrown) / 2;

    // ê°ì • íŒì •
    // ë†€ëŒ: ëˆˆ í¬ê²Œ ëœ¸ + ì… ë²Œë¦¼
    if (eyeOpenRatio > 0.08 && mouthOpenRatio > 0.15) {
      return 'surprised';
    }

    // í–‰ë³µ: ë¯¸ì†Œ (ì…ê¼¬ë¦¬ ì˜¬ë¼ê° + ì… ë„ˆë¹„ ë„“ìŒ)
    if (smileRatio > 0.01 && mouthWidthRatio > 0.9) {
      return 'happy';
    }

    // ë¶ˆì•ˆ/ê¸´ì¥: ëˆˆì¹ ì°Œí‘¸ë¦¼ + ì… ë‹¤ë¬¼ìŒ
    if (browFrownRatio > 0.05 && mouthOpenRatio < 0.05) {
      return 'anxious';
    }

    // í˜¼ë€: ëˆˆì¹ ë¹„ëŒ€ì¹­ ë˜ëŠ” ì•½í•œ ì°Œí‘¸ë¦¼
    const browAsymmetry = Math.abs(leftBrowFrown - rightBrowFrown);
    if (browAsymmetry > 0.02 || (browFrownRatio > 0.03 && browFrownRatio < 0.05)) {
      return 'confused';
    }

    // ìŠ¬í””: ì…ê¼¬ë¦¬ ë‚´ë ¤ê°
    if (smileRatio < -0.02) {
      return 'sad';
    }

    return 'neutral';
  }, []);

  // ê°ì • ê°ì§€ ë£¨í”„ - ì‹¤ì œ ì–¼êµ´ ì¸ì‹ ì‚¬ìš©
  const detectEmotion = useCallback(async () => {
    if (!isActive || !videoRef.current || !detectorRef.current) return;

    try {
      const video = videoRef.current;

      // ë¹„ë””ì˜¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìŠ¤í‚µ
      if (video.readyState < 2) return;

      // ì–¼êµ´ ëœë“œë§ˆí¬ ê°ì§€
      const faces = await detectorRef.current.estimateFaces(video, {
        flipHorizontal: false,
      });

      if (faces.length === 0) {
        // ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ
        console.log('[FaceDetection] No face detected');
        return;
      }

      const face = faces[0];
      const keypoints = face.keypoints;

      // ëœë“œë§ˆí¬ ê¸°ë°˜ ê°ì • ë¶„ì„
      const emotion = analyzeEmotionFromLandmarks(keypoints);
      const confidence = face.box ? 0.85 : 0.7; // ì–¼êµ´ ë°•ìŠ¤ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ë†’ìŒ

      setCurrentEmotion(emotion);

      // íƒ€ì„ë¼ì¸ì— ê¸°ë¡
      const record: EmotionRecord = {
        timestamp: Date.now(),
        emotion,
        confidence,
        questionIndex: currentQuestionIndexRef.current,
      };

      setEmotionTimeline(prev => [...prev, record]);

      // ì½œë°± í˜¸ì¶œ
      if (onEmotionChange) {
        onEmotionChange(emotion, confidence);
      }
    } catch (err) {
      console.warn('[FaceDetection] Detection error:', err);
    }
  }, [isActive, analyzeEmotionFromLandmarks, onEmotionChange]);

  // ê°ì§€ ì‹œì‘
  const startDetection = useCallback(async (): Promise<boolean> => {
    setError(null); // ì´ì „ ì—ëŸ¬ ì´ˆê¸°í™”
    setIsLoading(true);

    try {
      // TensorFlow.js ì´ˆê¸°í™”
      await initTensorFlow();

      // Face Landmarks Detection ëª¨ë¸ ë¡œë“œ (TensorFlow.js ë°±ì—”ë“œ)
      if (!detectorRef.current) {
        console.log('[FaceDetection] Loading face landmarks model...');
        const model = faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh;
        const detectorConfig: faceLandmarksDetection.MediaPipeFaceMeshTfjsModelConfig = {
          runtime: 'tfjs',
          refineLandmarks: true,
          maxFaces: 1,
        };
        detectorRef.current = await faceLandmarksDetection.createDetector(model, detectorConfig);
        console.log('[FaceDetection] Face landmarks model loaded');
      }

      // ì›¹ìº  ì‹œì‘
      const webcamStarted = await startWebcam();
      if (!webcamStarted) {
        setIsLoading(false);
        return false;
      }

      setIsActive(true);
      setIsLoading(false);

      return true;
    } catch (error) {
      console.error('[FaceDetection] Failed to start:', error);
      setError('ì–¼êµ´ ì¸ì‹ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      setIsLoading(false);
      return false;
    }
  }, [startWebcam]);

  // ê°ì§€ ì¤‘ì§€
  const stopDetection = useCallback(() => {
    setIsActive(false);

    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
      detectionIntervalRef.current = null;
    }

    stopWebcam();
  }, [stopWebcam]);

  // íƒ€ì„ë¼ì¸ ì´ˆê¸°í™”
  const clearTimeline = useCallback(() => {
    setEmotionTimeline([]);
  }, []);

  // íŠ¹ì • ë¬¸í•­ì˜ ê°ì • ê¸°ë¡
  const recordEmotionForQuestion = useCallback((questionIndex: number) => {
    currentQuestionIndexRef.current = questionIndex;
  }, []);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopDetection();
    };
  }, [stopDetection]);

  // ê°ì§€ ë£¨í”„ ì—…ë°ì´íŠ¸ - async í•¨ìˆ˜ ì²˜ë¦¬
  useEffect(() => {
    if (!isActive) {
      if (detectionIntervalRef.current) {
        clearInterval(detectionIntervalRef.current);
        detectionIntervalRef.current = null;
      }
      return;
    }

    // async í•¨ìˆ˜ë¥¼ ì¸í„°ë²Œì—ì„œ ì‹¤í–‰
    const runDetection = () => {
      detectEmotion();
    };

    detectionIntervalRef.current = setInterval(runDetection, detectionInterval);

    return () => {
      if (detectionIntervalRef.current) {
        clearInterval(detectionIntervalRef.current);
        detectionIntervalRef.current = null;
      }
    };
  }, [isActive, detectEmotion, detectionInterval]);

  // ë¹„ë””ì˜¤ ìš”ì†Œê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œ ìŠ¤íŠ¸ë¦¼ ì¬í• ë‹¹
  // (í™”ë©´ ì „í™˜ìœ¼ë¡œ ìƒˆ video ìš”ì†Œê°€ ë§ˆìš´íŠ¸ë  ë•Œ í•„ìš”)
  useEffect(() => {
    if (!isActive) return;

    const checkAndAssignStream = () => {
      if (streamRef.current && videoRef.current) {
        // í˜„ì¬ ë¹„ë””ì˜¤ ìš”ì†Œì— ìŠ¤íŠ¸ë¦¼ì´ ì—†ê±°ë‚˜ ë‹¤ë¥¸ ìŠ¤íŠ¸ë¦¼ì´ë©´ ì¬í• ë‹¹
        if (videoRef.current.srcObject !== streamRef.current) {
          console.log('[FaceDetection] Re-assigning stream to new video element');
          videoRef.current.srcObject = streamRef.current;
          videoRef.current.style.filter = 'none';
          videoRef.current.play().catch(err => {
            console.warn('[FaceDetection] Video play failed:', err);
          });
        }
      }
    };

    // ì¦‰ì‹œ ì²´í¬
    checkAndAssignStream();

    // ì¹´ë©”ë¼ í™œì„±í™” ì¤‘ì—ëŠ” ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬ (í™”ë©´ ì „í™˜ ê°ì§€)
    // ìŠ¤íŠ¸ë¦¼ì´ ì •ìƒ í• ë‹¹ë˜ë©´ ì²´í¬ê°€ ë¹ ë¥´ê²Œ ì¢…ë£Œë¨
    const interval = setInterval(checkAndAssignStream, 200);

    return () => clearInterval(interval);
  }, [isActive]);

  return {
    isLoading,
    isActive,
    isPermissionGranted,
    currentEmotion,
    emotionTimeline,
    emotionDistribution,
    dominantEmotion,
    error,
    videoRef,
    startDetection,
    stopDetection,
    clearTimeline,
    recordEmotionForQuestion,
  };
}

// ê°ì • ë¶„í¬ ê³„ì‚° ìœ í‹¸ë¦¬í‹°
function calculateEmotionDistribution(timeline: EmotionRecord[]): EmotionDistribution[] {
  if (timeline.length === 0) return [];

  const counts: Record<EmotionType, number> = {
    neutral: 0,
    happy: 0,
    confused: 0,
    anxious: 0,
    sad: 0,
    surprised: 0,
    angry: 0,
  };

  timeline.forEach(record => {
    counts[record.emotion]++;
  });

  const total = timeline.length;

  return Object.entries(counts)
    .filter(([, count]) => count > 0)
    .map(([emotion, count]) => ({
      emotion: emotion as EmotionType,
      count,
      percentage: Math.round((count / total) * 100),
    }))
    .sort((a, b) => b.count - a.count);
}

// ê°ì • ì´ë¦„ í•œê¸€í™”
export const emotionLabels: Record<EmotionType, string> = {
  neutral: 'ì§‘ì¤‘',
  happy: 'ë§Œì¡±',
  confused: 'í˜¼ë€',
  anxious: 'ë¶ˆì•ˆ',
  sad: 'ìŠ¬í””',
  surprised: 'ë†€ëŒ',
  angry: 'í™”ë‚¨',
};

// ê°ì • ì•„ì´ì½˜
export const emotionIcons: Record<EmotionType, string> = {
  neutral: 'ğŸ˜',
  happy: 'ğŸ˜Š',
  confused: 'ğŸ˜•',
  anxious: 'ğŸ˜°',
  sad: 'ğŸ˜¢',
  surprised: 'ğŸ˜®',
  angry: 'ğŸ˜ ',
};

// ê°ì • ìƒ‰ìƒ
export const emotionColors: Record<EmotionType, string> = {
  neutral: '#6B7280',
  happy: '#10B981',
  confused: '#F59E0B',
  anxious: '#EF4444',
  sad: '#3B82F6',
  surprised: '#8B5CF6',
  angry: '#DC2626',
};

export default useFaceDetection;
