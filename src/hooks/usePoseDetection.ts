'use client';

import { useState, useEffect, useCallback, useRef } from 'react';
import { initTensorFlow } from '@/lib/ai/tensorflow';
import * as poseDetection from '@tensorflow-models/pose-detection';

// í‚¤í¬ì¸íŠ¸ íƒ€ì…
export interface Keypoint {
  x: number;
  y: number;
  score?: number;
  name?: string;
}

// ìì„¸ íƒ€ì…
export type PostureType =
  | 'upright'     // ë°”ë¥¸ ìì„¸
  | 'leaning_left'  // ì™¼ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì§
  | 'leaning_right' // ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì§
  | 'slouching'   // êµ¬ë¶€ì •í•œ ìì„¸
  | 'unknown';    // ê°ì§€ ë¶ˆê°€

// ìì„¸ ê¸°ë¡
export interface PostureRecord {
  timestamp: number;
  posture: PostureType;
  tiltAngle: number; // ê¸°ìš¸ê¸° ê°ë„ (ë„)
  questionIndex?: number;
}

// ìì„¸ í†µê³„
export interface PostureStats {
  uprightPercentage: number;
  leftTiltPercentage: number;
  rightTiltPercentage: number;
  slouchingPercentage: number;
  totalTiltCount: number;
  avgTiltDuration: number; // ms
}

// í›… ì˜µì…˜
interface UsePoseDetectionOptions {
  enabled?: boolean;
  detectionInterval?: number;
  tiltThreshold?: number; // ê¸°ìš¸ê¸° ì„ê³„ê°’ (ë„)
  onPostureChange?: (posture: PostureType, angle: number) => void;
  onKeypointsDetected?: (keypoints: Keypoint[]) => void;
}

// í›… ë°˜í™˜ íƒ€ì…
interface UsePoseDetectionReturn {
  isLoading: boolean;
  isActive: boolean;
  currentPosture: PostureType;
  currentTiltAngle: number;
  postureTimeline: PostureRecord[];
  postureStats: PostureStats;
  keypoints: Keypoint[];
  videoRef: React.RefObject<HTMLVideoElement | null>;
  canvasRef: React.RefObject<HTMLCanvasElement | null>;
  startDetection: () => Promise<boolean>;
  stopDetection: () => void;
  clearTimeline: () => void;
  recordPostureForQuestion: (questionIndex: number) => void;
  drawKeypoints: () => void;
}

// MoveNet í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤
const KEYPOINT_NAMES = [
  'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
  'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
  'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
  'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
];

// ìŠ¤ì¼ˆë ˆí†¤ ì—°ê²° ì •ì˜ (í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤ ìŒ)
const SKELETON_CONNECTIONS: [number, number][] = [
  [0, 1], [0, 2],           // nose -> eyes
  [1, 3], [2, 4],           // eyes -> ears
  [5, 6],                   // shoulders
  [5, 7], [7, 9],           // left arm
  [6, 8], [8, 10],          // right arm
  [5, 11], [6, 12],         // torso
  [11, 12],                 // hips
  [11, 13], [13, 15],       // left leg
  [12, 14], [14, 16],       // right leg
];

export function usePoseDetection(options: UsePoseDetectionOptions = {}): UsePoseDetectionReturn {
  const {
    enabled = true,
    detectionInterval = 100, // ë” ë¹ ë¥¸ ê°ì§€ë¥¼ ìœ„í•´ 100msë¡œ ë³€ê²½
    tiltThreshold = 15,
    onPostureChange,
    onKeypointsDetected,
  } = options;

  const [isLoading, setIsLoading] = useState(false);
  const [isActive, setIsActive] = useState(false);
  const [currentPosture, setCurrentPosture] = useState<PostureType>('unknown');
  const [currentTiltAngle, setCurrentTiltAngle] = useState(0);
  const [postureTimeline, setPostureTimeline] = useState<PostureRecord[]>([]);
  const [keypoints, setKeypoints] = useState<Keypoint[]>([]);

  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const detectorRef = useRef<poseDetection.PoseDetector | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const currentQuestionIndexRef = useRef<number>(0);

  // ìì„¸ í†µê³„ ê³„ì‚°
  const postureStats = calculatePostureStats(postureTimeline);

  // ì›¹ìº  ìŠ¤íŠ¸ë¦¼ ì‹œì‘
  const startWebcam = useCallback(async (): Promise<boolean> => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640 },
          height: { ideal: 480 },
          facingMode: 'user',
        },
        audio: false,
      });

      const videoTrack = stream.getVideoTracks()[0];
      if (videoTrack) {
        const settings = videoTrack.getSettings();
        console.log('[PoseDetection] Video track settings:', settings);
      }

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        videoRef.current.style.filter = 'none';
        await videoRef.current.play();
      }

      streamRef.current = stream;
      return true;
    } catch (error) {
      console.error('[PoseDetection] Webcam access denied:', error);
      return false;
    }
  }, []);

  // ì›¹ìº  ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€
  const stopWebcam = useCallback(() => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
  }, []);

  // MoveNet ëª¨ë¸ ì´ˆê¸°í™”
  const initDetector = useCallback(async (): Promise<boolean> => {
    try {
      console.log('[PoseDetection] Initializing MoveNet detector...');

      const detector = await poseDetection.createDetector(
        poseDetection.SupportedModels.MoveNet,
        {
          modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING,
          enableSmoothing: true,
        }
      );

      detectorRef.current = detector;
      console.log('[PoseDetection] MoveNet detector initialized');
      return true;
    } catch (error) {
      console.error('[PoseDetection] Failed to initialize detector:', error);
      return false;
    }
  }, []);

  // í‚¤í¬ì¸íŠ¸ë¥¼ ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸°
  const drawKeypoints = useCallback(() => {
    const canvas = canvasRef.current;
    const video = videoRef.current;
    if (!canvas || !video || keypoints.length === 0) return;

    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    // ìº”ë²„ìŠ¤ í¬ê¸°ë¥¼ ë¹„ë””ì˜¤ì— ë§ì¶¤
    canvas.width = video.videoWidth || 640;
    canvas.height = video.videoHeight || 480;

    // ìº”ë²„ìŠ¤ ì´ˆê¸°í™”
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    // ë¯¸ëŸ¬ë§ì„ ìœ„í•œ ë³€í™˜
    ctx.save();
    ctx.scale(-1, 1);
    ctx.translate(-canvas.width, 0);

    // ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸° (ì„ )
    ctx.strokeStyle = '#00FF00';
    ctx.lineWidth = 3;

    for (const [startIdx, endIdx] of SKELETON_CONNECTIONS) {
      const start = keypoints[startIdx];
      const end = keypoints[endIdx];

      if (start && end && (start.score || 0) > 0.3 && (end.score || 0) > 0.3) {
        ctx.beginPath();
        ctx.moveTo(start.x, start.y);
        ctx.lineTo(end.x, end.y);
        ctx.stroke();
      }
    }

    // í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (ì›)
    for (const kp of keypoints) {
      if ((kp.score || 0) > 0.3) {
        // ì™¸ê³½ì„ 
        ctx.beginPath();
        ctx.arc(kp.x, kp.y, 8, 0, 2 * Math.PI);
        ctx.fillStyle = '#00FF00';
        ctx.fill();

        // ë‚´ë¶€
        ctx.beginPath();
        ctx.arc(kp.x, kp.y, 5, 0, 2 * Math.PI);
        ctx.fillStyle = '#FFFFFF';
        ctx.fill();
      }
    }

    ctx.restore();
  }, [keypoints]);

  // ìì„¸ ë¶„ì„ (ì‹¤ì œ í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
  const analyzePostureFromKeypoints = useCallback((kps: Keypoint[]): { posture: PostureType; angle: number } => {
    // ì–´ê¹¨ í‚¤í¬ì¸íŠ¸ ì°¾ê¸°
    const leftShoulder = kps[5];
    const rightShoulder = kps[6];

    if (!leftShoulder || !rightShoulder ||
        (leftShoulder.score || 0) < 0.3 || (rightShoulder.score || 0) < 0.3) {
      return { posture: 'unknown', angle: 0 };
    }

    // ì–´ê¹¨ ê¸°ìš¸ê¸° ê³„ì‚°
    const deltaY = rightShoulder.y - leftShoulder.y;
    const deltaX = rightShoulder.x - leftShoulder.x;
    const angle = Math.atan2(deltaY, deltaX) * (180 / Math.PI);

    let posture: PostureType;
    if (Math.abs(angle) > tiltThreshold) {
      posture = angle > 0 ? 'leaning_right' : 'leaning_left';
    } else {
      posture = 'upright';
    }

    return { posture, angle };
  }, [tiltThreshold]);

  // ì‹¤ì‹œê°„ í¬ì¦ˆ ê°ì§€ ë£¨í”„
  const detectPose = useCallback(async () => {
    if (!isActive || !videoRef.current || !detectorRef.current) {
      return;
    }

    try {
      const poses = await detectorRef.current.estimatePoses(videoRef.current);

      if (poses.length > 0 && poses[0].keypoints) {
        const detectedKeypoints = poses[0].keypoints.map((kp, idx) => ({
          x: kp.x,
          y: kp.y,
          score: kp.score,
          name: KEYPOINT_NAMES[idx],
        }));

        setKeypoints(detectedKeypoints);

        // ì½œë°± í˜¸ì¶œ
        if (onKeypointsDetected) {
          onKeypointsDetected(detectedKeypoints);
        }

        // ìì„¸ ë¶„ì„
        const { posture, angle } = analyzePostureFromKeypoints(detectedKeypoints);

        if (posture !== 'unknown') {
          setCurrentPosture(posture);
          setCurrentTiltAngle(angle);

          // íƒ€ì„ë¼ì¸ì— ê¸°ë¡
          const record: PostureRecord = {
            timestamp: Date.now(),
            posture,
            tiltAngle: angle,
            questionIndex: currentQuestionIndexRef.current,
          };
          setPostureTimeline(prev => [...prev.slice(-100), record]); // ìµœê·¼ 100ê°œë§Œ ìœ ì§€

          if (onPostureChange) {
            onPostureChange(posture, angle);
          }
        }
      }
    } catch (error) {
      console.error('[PoseDetection] Detection error:', error);
    }

    // ë‹¤ìŒ í”„ë ˆì„ ì˜ˆì•½
    if (isActive) {
      animationFrameRef.current = requestAnimationFrame(detectPose);
    }
  }, [isActive, analyzePostureFromKeypoints, onKeypointsDetected, onPostureChange]);

  // í‚¤í¬ì¸íŠ¸ ë³€ê²½ ì‹œ ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸°
  useEffect(() => {
    if (keypoints.length > 0) {
      drawKeypoints();
    }
  }, [keypoints, drawKeypoints]);

  // ê°ì§€ ì‹œì‘
  const startDetection = useCallback(async (): Promise<boolean> => {
    setIsLoading(true);

    try {
      // TensorFlow.js ì´ˆê¸°í™”
      await initTensorFlow();

      // ì›¹ìº  ì‹œì‘
      const webcamStarted = await startWebcam();
      if (!webcamStarted) {
        setIsLoading(false);
        return false;
      }

      // ë¹„ë””ì˜¤ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
      await new Promise<void>((resolve) => {
        if (videoRef.current) {
          if (videoRef.current.readyState >= 2) {
            resolve();
          } else {
            videoRef.current.onloadeddata = () => resolve();
          }
        } else {
          resolve();
        }
      });

      // MoveNet ì´ˆê¸°í™”
      const detectorInitialized = await initDetector();
      if (!detectorInitialized) {
        console.warn('[PoseDetection] Using simulation mode');
      }

      setIsActive(true);
      setIsLoading(false);

      // ê°ì§€ ë£¨í”„ ì‹œì‘
      animationFrameRef.current = requestAnimationFrame(detectPose);

      return true;
    } catch (error) {
      console.error('[PoseDetection] Failed to start:', error);
      setIsLoading(false);
      return false;
    }
  }, [startWebcam, initDetector, detectPose]);

  // ê°ì§€ ì¤‘ì§€
  const stopDetection = useCallback(() => {
    setIsActive(false);

    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    if (detectorRef.current) {
      detectorRef.current.dispose();
      detectorRef.current = null;
    }

    stopWebcam();
    setKeypoints([]);
  }, [stopWebcam]);

  // íƒ€ì„ë¼ì¸ ì´ˆê¸°í™”
  const clearTimeline = useCallback(() => {
    setPostureTimeline([]);
  }, []);

  // íŠ¹ì • ë¬¸í•­ì˜ ìì„¸ ê¸°ë¡
  const recordPostureForQuestion = useCallback((questionIndex: number) => {
    currentQuestionIndexRef.current = questionIndex;
  }, []);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (detectorRef.current) {
        detectorRef.current.dispose();
      }
      stopWebcam();
    };
  }, [stopWebcam]);

  // isActive ë³€ê²½ ì‹œ ê°ì§€ ë£¨í”„ ê´€ë¦¬
  useEffect(() => {
    if (isActive && !animationFrameRef.current) {
      animationFrameRef.current = requestAnimationFrame(detectPose);
    }
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }
    };
  }, [isActive, detectPose]);

  return {
    isLoading,
    isActive,
    currentPosture,
    currentTiltAngle,
    postureTimeline,
    postureStats,
    keypoints,
    videoRef,
    canvasRef,
    startDetection,
    stopDetection,
    clearTimeline,
    recordPostureForQuestion,
    drawKeypoints,
  };
}

// ìì„¸ í†µê³„ ê³„ì‚° ìœ í‹¸ë¦¬í‹°
function calculatePostureStats(timeline: PostureRecord[]): PostureStats {
  if (timeline.length === 0) {
    return {
      uprightPercentage: 0,
      leftTiltPercentage: 0,
      rightTiltPercentage: 0,
      slouchingPercentage: 0,
      totalTiltCount: 0,
      avgTiltDuration: 0,
    };
  }

  const total = timeline.length;
  let upright = 0;
  let leftTilt = 0;
  let rightTilt = 0;
  let slouching = 0;

  let tiltSequences: number[] = [];
  let currentTiltStart: number | null = null;

  timeline.forEach((record, index) => {
    switch (record.posture) {
      case 'upright':
        upright++;
        if (currentTiltStart !== null) {
          tiltSequences.push(record.timestamp - currentTiltStart);
          currentTiltStart = null;
        }
        break;
      case 'leaning_left':
        leftTilt++;
        if (currentTiltStart === null) {
          currentTiltStart = record.timestamp;
        }
        break;
      case 'leaning_right':
        rightTilt++;
        if (currentTiltStart === null) {
          currentTiltStart = record.timestamp;
        }
        break;
      case 'slouching':
        slouching++;
        break;
    }

    if (index === timeline.length - 1 && currentTiltStart !== null) {
      tiltSequences.push(record.timestamp - currentTiltStart);
    }
  });

  const avgTiltDuration = tiltSequences.length > 0
    ? tiltSequences.reduce((a, b) => a + b, 0) / tiltSequences.length
    : 0;

  return {
    uprightPercentage: Math.round((upright / total) * 100),
    leftTiltPercentage: Math.round((leftTilt / total) * 100),
    rightTiltPercentage: Math.round((rightTilt / total) * 100),
    slouchingPercentage: Math.round((slouching / total) * 100),
    totalTiltCount: tiltSequences.length,
    avgTiltDuration: Math.round(avgTiltDuration),
  };
}

// ìì„¸ ì´ë¦„ í•œê¸€í™”
export const postureLabels: Record<PostureType, string> = {
  upright: 'ë°”ë¥¸ ìì„¸',
  leaning_left: 'ì™¼ìª½ ê¸°ìš¸ì„',
  leaning_right: 'ì˜¤ë¥¸ìª½ ê¸°ìš¸ì„',
  slouching: 'êµ¬ë¶€ì •í•¨',
  unknown: 'ê°ì§€ ë¶ˆê°€',
};

// ìì„¸ ì•„ì´ì½˜
export const postureIcons: Record<PostureType, string> = {
  upright: 'ğŸ§˜',
  leaning_left: 'â†–ï¸',
  leaning_right: 'â†—ï¸',
  slouching: 'ğŸª‘',
  unknown: 'â“',
};

// ìì„¸ ìƒ‰ìƒ
export const postureColors: Record<PostureType, string> = {
  upright: '#10B981',
  leaning_left: '#F59E0B',
  leaning_right: '#F59E0B',
  slouching: '#EF4444',
  unknown: '#6B7280',
};

// ëª©í‘œ ìì„¸ ì •ì˜
export interface TargetPose {
  name: string;
  keypoints: Record<string, { x: number; y: number; minConfidence?: number }>;
  tolerance: number;
}

// ìì„¸ ë¹„êµ ê²°ê³¼
export interface PoseComparisonResult {
  isMatching: boolean;
  matchScore: number;
  matchedPoints: string[];
  unmatchedPoints: string[];
}

/**
 * ëª©í‘œ ìì„¸ì™€ í˜„ì¬ ìì„¸ ë¹„êµ
 */
export function comparePose(
  target: TargetPose,
  currentKeypoints: Record<string, { x: number; y: number; confidence: number }>
): PoseComparisonResult {
  const targetPoints = Object.keys(target.keypoints);
  const matchedPoints: string[] = [];
  const unmatchedPoints: string[] = [];

  for (const pointName of targetPoints) {
    const targetPoint = target.keypoints[pointName];
    const currentPoint = currentKeypoints[pointName];

    if (!currentPoint) {
      unmatchedPoints.push(pointName);
      continue;
    }

    const minConfidence = targetPoint.minConfidence || 0.5;
    if (currentPoint.confidence < minConfidence) {
      unmatchedPoints.push(pointName);
      continue;
    }

    const distance = Math.sqrt(
      Math.pow(currentPoint.x - targetPoint.x, 2) +
      Math.pow(currentPoint.y - targetPoint.y, 2)
    );

    if (distance <= target.tolerance) {
      matchedPoints.push(pointName);
    } else {
      unmatchedPoints.push(pointName);
    }
  }

  const matchScore = targetPoints.length > 0
    ? Math.round((matchedPoints.length / targetPoints.length) * 100)
    : 0;

  return {
    isMatching: matchScore >= 70,
    matchScore,
    matchedPoints,
    unmatchedPoints,
  };
}

/**
 * ê°„ë‹¨í•œ ë™ì‘ ê°ì§€ (ì‹œë®¬ë ˆì´ì…˜ìš©)
 */
export function detectSimpleMovement(
  movementType: string,
  _videoElement: HTMLVideoElement | null
): { detected: boolean; confidence: number } {
  const detected = Math.random() > 0.2;
  const confidence = detected ? 0.7 + Math.random() * 0.3 : 0.2 + Math.random() * 0.3;
  return { detected, confidence };
}

/**
 * ë™ì‘ ì¼ì¹˜ë„ ì ìˆ˜ ê³„ì‚°
 */
export function calculateMovementScore(
  targetMovement: string,
  detectionHistory: Array<{ detected: boolean; confidence: number; timestamp: number }>,
  duration: number
): number {
  if (detectionHistory.length === 0) return 0;

  const successRate = detectionHistory.filter(d => d.detected).length / detectionHistory.length;
  const avgConfidence = detectionHistory.reduce((sum, d) => sum + d.confidence, 0) / detectionHistory.length;
  const baseScore = successRate * 80 + avgConfidence * 20;

  return Math.round(Math.min(baseScore, 100));
}

export default usePoseDetection;
